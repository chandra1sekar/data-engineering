<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Week 07 - sync session">
  <title>Fundamentals of Data Engineering</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="reveal.js/css/theme/mids.css" id="theme">
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section>
  <h1 class="title">Fundamentals of Data Engineering</h1>
  <h2 class="author">Week 07 - sync session</h2>
  <img class="frontPageSlogan" src="http://people.ischool.berkeley.edu/~mark.mims/course-development/2017-mids-w205/media/datascience-at-berkeley.png"/>
</section>

<section class="slide level1">

<section id="while-were-getting-started" class="level2">
<h2>While we’re getting started</h2>
<ul>
<li>Review your Assignment 06</li>
<li>Get ready to share</li>
</ul>
<aside class="notes">
<p>Breakout at about 5 after the hour: - Check in with each group - have students share screen</p>
</aside>
</section>
<section id="due-friday-pr" class="level2">
<h2>Due Friday (PR)</h2>
</section>
</section>
<section id="section" class="slide level1">
<h1></h1>
<section id="section-1" class="level2">
<h2></h2>
<p><img data-src="images/streaming-bare.svg" style="border:0;box-shadow:none" /></p>
<aside class="notes">
<ul>
<li>Last week we published and consumed messages with kafka</li>
<li>Now we’ll consume messages with Spark and take a look at them</li>
<li>Next week, we’ll transform them in spark so we can land them in hdfs</li>
</ul>
</aside>
</section>
</section>
<section id="section-2" class="slide level1">
<h1></h1>
<section id="spark-stack-with-kafka" class="level2">
<h2>Spark Stack with Kafka</h2>
</section>
<section id="setup" class="level2">
<h2>Setup</h2>
</section>
<section id="docker-compose.yml" class="level2">
<h2><code>docker-compose.yml</code></h2>
<pre><code>---
version: &#39;2&#39;
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
    expose:
      - &quot;2181&quot;
      - &quot;2888&quot;
      - &quot;32181&quot;
      - &quot;3888&quot;
    #ports:
      #- &quot;32181:32181&quot;
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    expose:
      - &quot;9092&quot;
      - &quot;29092&quot;
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;

  myspark:
    image: midsw205/spark-python:latest
    stdin_open: true
    tty: true
    volumes:
      - ~/w205:/w205
    command: bash
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;</code></pre>
<aside class="notes">
<p>and save a data file.</p>
</aside>
</section>
<section id="up" class="level2">
<h2>up</h2>
<pre><code>docker-compose up -d

docker-compose logs -f kafka</code></pre>
<aside class="notes">
<p>Now spin up the cluster</p>
<pre><code>docker-compose up -d</code></pre>
<p>and watch it come up</p>
<pre><code>docker-compose logs -f kafka</code></pre>
<p>when this looks like it’s done, you can safely detach with <code>Ctrl-C</code>.</p>
</aside>
</section>
<section id="use-it" class="level2">
<h2>use it</h2>
</section>
<section id="create-a-topic" class="level2">
<h2>create a topic</h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
    --create \
    --topic foo \
    --partitions 1 \
    --replication-factor 1 \
    --if-not-exists \
    --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<p>First, create a topic <code>foo</code></p>
<p>docker-compose exec kafka kafka-topics –create –topic foo –partitions 1 –replication-factor 1 –if-not-exists –zookeeper zookeeper:32181</p>
</aside>
</section>
<section id="should-show" class="level2">
<h2>Should show</h2>
<pre><code>Created topic &quot;foo&quot;.</code></pre>
</section>
<section id="check-the-topic" class="level2">
<h2>Check the topic</h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
  --describe \
  --topic foo \
  --zookeeper zookeeper:32181</code></pre>
</section>
<section id="should-show-1" class="level2">
<h2>Should show</h2>
<pre><code>Topic:foo   PartitionCount:1    ReplicationFactor:1 Configs:
Topic: foo  Partition: 0    Leader: 1    Replicas: 1  Isr: 1</code></pre>
</section>
<section id="publish-some-stuff-to-kafka" class="level2">
<h2>Publish some stuff to kafka</h2>
<pre><code>docker-compose exec kafka \
  bash -c &quot;seq 42 | kafka-console-producer \
    --request-required-acks 1 \
    --broker-list kafka:29092 \
    --topic foo &amp;&amp; echo &#39;Produced 42 messages.&#39;&quot;</code></pre>
<aside class="notes">
<p>Use the kafka console producer to publish some test messages to that topic</p>
<p>docker-compose exec kafka bash -c “seq 42 | kafka-console-producer –request-required-acks 1 –broker-list kafka:29092 –topic foo &amp;&amp; echo ‘Produced 42 messages.’”</p>
</aside>
</section>
<section id="should-show-2" class="level2">
<h2>Should show</h2>
<pre><code>Produced 42 messages.</code></pre>
</section>
</section>
<section id="section-3" class="slide level1">
<h1></h1>
<section id="run-spark-using-the-myspark-container" class="level2">
<h2>Run spark using the <code>myspark</code> container</h2>
<pre><code>docker-compose exec myspark pyspark</code></pre>
<aside class="notes">
<p>Spin up a pyspark process using the <code>myspark</code> container</p>
<p>docker-compose exec myspark pyspark</p>
<p>We have to add some kafka library dependencies on the cli for now.</p>
</aside>
</section>
<section id="read-stuff-from-kafka" class="level2">
<h2>read stuff from kafka</h2>
<p>At the pyspark prompt,</p>
<pre><code>numbers = spark \
  .read \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \
  .option(&quot;subscribe&quot;,&quot;foo&quot;) \
  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \
  .option(&quot;endingOffsets&quot;, &quot;latest&quot;) \
  .load() </code></pre>
<aside class="notes">
<p>At the pyspark prompt,</p>
<p>read from kafka</p>
<p>numbers = spark<br />
.read<br />
.format(“kafka”)<br />
.option(“kafka.bootstrap.servers”, “kafka:29092”)<br />
.option(“subscribe”,“foo”)<br />
.option(“startingOffsets”, “earliest”)<br />
.option(“endingOffsets”, “latest”)<br />
.load()</p>
</aside>
</section>
<section id="see-the-schema" class="level2">
<h2>See the schema</h2>
<pre><code>numbers.printSchema()</code></pre>
</section>
<section id="cast-it-as-strings" class="level2">
<h2>Cast it as strings</h2>
<pre><code>numbers_as_strings=numbers.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</code></pre>
<aside class="notes">
<p>cast it as strings (you can totally use <code>INT</code>s if you’d like)</p>
<p>numbers_as_strings=numbers.selectExpr(“CAST(key AS STRING)”, “CAST(value AS STRING)”)</p>
</aside>
</section>
<section id="take-a-look" class="level2">
<h2>Take a look</h2>
<pre><code>numbers_as_strings.show()</code></pre>
<pre><code>numbers_as_strings.printSchema()</code></pre>
<pre><code>numbers_as_strings.count()</code></pre>
<aside class="notes">
<p>then you can exit pyspark using either <code>ctrl-d</code> or <code>exit()</code>.</p>
<p><code>numbers_as_strings.show()</code></p>
<p>numbers_as_strings.printSchema()</p>
</aside>
</section>
<section id="down" class="level2">
<h2>down</h2>
<pre><code>docker-compose down</code></pre>
</section>
</section>
<section id="section-4" class="slide level1">
<h1></h1>
<section id="spark-stack-with-kafka-with-real-messages" class="level2">
<h2>Spark stack with Kafka with “real” messages</h2>
<aside class="notes">

</aside>
</section>
<section id="docker-compose.yml-file" class="level2">
<h2>docker-compose.yml file</h2>
<pre><code>---
version: &#39;2&#39;
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 32181
      ZOOKEEPER_TICK_TIME: 2000
    expose:
      - &quot;2181&quot;
      - &quot;2888&quot;
      - &quot;32181&quot;
      - &quot;3888&quot;
    #ports:
      #- &quot;32181:32181&quot;
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    volumes:
      - ~/w205:/w205
    expose:
      - &quot;9092&quot;
      - &quot;29092&quot;
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;

  myhdfs:
    image: midsw205/cdh-minimal:latest
    expose:
      - &quot;8020&quot; # nn
      - &quot;50070&quot; # nn http
      - &quot;8888&quot; # hue
    #ports:
    #- &quot;8888:8888&quot;
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;

  myspark:
    image: midsw205/spark-python:latest
    stdin_open: true
    tty: true
    volumes:
      - ~/w205:/w205
    command: bash
    depends_on:
      - myhdfs
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;

  mids:
    image: midsw205/base:latest
    stdin_open: true
    tty: true
    volumes:
      - ~/w205:/w205
    extra_hosts:
      - &quot;moby:127.0.0.1&quot;</code></pre>
<aside class="notes">
<ul>
<li>See ~/w205/spark-with-kafka-json folder’s docker-compose.yml</li>
<li>may use this cluster, may have it working w/o myhdfs container</li>
</ul>
</aside>
</section>
<section id="pull-data" class="level2">
<h2>Pull data</h2>
<pre><code>curl -L -o github-example-large.json https://goo.gl/Hr6erG</code></pre>
</section>
<section id="spin-up-the-cluster" class="level2">
<h2>Spin up the cluster</h2>
<pre><code>docker-compose up -d</code></pre>
</section>
<section id="watch-it-come-up" class="level2">
<h2>Watch it come up</h2>
<pre><code>docker-compose logs -f kafka</code></pre>
<ul>
<li>Detach with <code>Ctrl-C</code></li>
</ul>
<aside class="notes">
<p>when this looks like it’s done, detach</p>
</aside>
</section>
<section id="use-it-1" class="level2">
<h2>use it</h2>
</section>
<section id="create-a-topic-1" class="level2">
<h2>create a topic</h2>
<pre><code>
docker-compose exec kafka \
  kafka-topics \
    --create \
    --topic foo \
    --partitions 1 \
    --replication-factor 1 \
    --if-not-exists \
    --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<p>docker-compose exec kafka kafka-topics –create –topic foo –partitions 1 –replication-factor 1 –if-not-exists –zookeeper zookeeper:32181</p>
</aside>
</section>
<section id="should-see-something-like" class="level2">
<h2>Should see something like</h2>
<pre><code>Created topic &quot;foo&quot;.</code></pre>
</section>
<section id="check-the-topic-1" class="level2">
<h2>Check the topic</h2>
<pre><code>docker-compose exec kafka \
  kafka-topics \
    --describe \
    --topic foo \
    --zookeeper zookeeper:32181</code></pre>
<aside class="notes">
<p>docker-compose exec kafka kafka-topics –describe –topic foo –zookeeper zookeeper:32181</p>
</aside>
</section>
<section id="should-see-something-like-1" class="level2">
<h2>Should see something like</h2>
<pre><code>Topic:foo   PartitionCount:1    ReplicationFactor:1 Configs:
Topic: foo  Partition: 0    Leader: 1    Replicas: 1  Isr: 1</code></pre>
</section>
</section>
<section id="section-5" class="slide level1">
<h1></h1>
<section id="publish-some-stuff-to-kafka-1" class="level2">
<h2>Publish some stuff to kafka</h2>
</section>
<section id="check-out-our-messages" class="level2">
<h2>Check out our messages</h2>
<pre><code>docker-compose exec mids bash -c &quot;cat /w205/github-example-large.json&quot;
docker-compose exec mids bash -c &quot;cat /w205/github-example-large.json | jq &#39;.&#39;&quot;</code></pre>
<aside class="notes">
<p>ugly 1st pretty print</p>
</aside>
</section>
<section id="individual-messages" class="level2">
<h2>Individual messages</h2>
<pre><code>docker-compose exec mids bash -c &quot;cat /w205/github-example-large.json | jq &#39;.[]&#39; -c&quot;</code></pre>
<aside class="notes">
<p>Go over | jq stuff</p>
</aside>
</section>
<section id="publish-some-test-messages-to-that-topic-with-the-kafka-console-producer" class="level2">
<h2>Publish some test messages to that topic with the kafka console producer</h2>
<pre><code>docker-compose exec mids \
  bash -c &quot;cat /w205/github-example-large.json \
    | jq &#39;.[]&#39; -c \
    | kafkacat -P -b kafka:29092 -t foo &amp;&amp; echo &#39;Produced 100 messages.&#39;&quot;</code></pre>
<aside class="notes">
<p>docker-compose exec mids bash -c “cat /w205/github-example-large.json | jq ‘.<a href="#/section-9"></a>’ -c | kafkacat -P -b kafka:29092 -t foo &amp;&amp; echo ‘Produced 100 messages.’”</p>
</aside>
</section>
<section id="should-see-something-like-2" class="level2">
<h2>Should see something like</h2>
<pre><code>Produced 100 messages.</code></pre>
</section>
</section>
<section id="section-6" class="slide level1">
<h1></h1>
<section id="run-spark-using-the-myspark-container-1" class="level2">
<h2>Run spark using the <code>myspark</code> container</h2>
<pre><code>docker-compose exec myspark pyspark</code></pre>
<aside class="notes">
<p>Spin up a pyspark process using the <code>myspark</code> container</p>
<p>docker-compose exec myspark pyspark</p>
<p>We have to add some kafka library dependencies on the cli for now.</p>
</aside>
</section>
<section id="read-stuff-from-kafka-1" class="level2">
<h2>read stuff from kafka</h2>
<p>At the pyspark prompt,</p>
<pre><code>messages = spark \
  .read \
  .format(&quot;kafka&quot;) \
  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \
  .option(&quot;subscribe&quot;,&quot;foo&quot;) \
  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \
  .option(&quot;endingOffsets&quot;, &quot;latest&quot;) \
  .load() </code></pre>
<aside class="notes">
<p>At the pyspark prompt,</p>
<p>read from kafka</p>
<p>messages = spark<br />
.read<br />
.format(“kafka”)<br />
.option(“kafka.bootstrap.servers”, “kafka:29092”)<br />
.option(“subscribe”,“foo”)<br />
.option(“startingOffsets”, “earliest”)<br />
.option(“endingOffsets”, “latest”)<br />
.load()</p>
<ul>
<li>NOTE: change the name of the df - gitmessages or something</li>
</ul>
</aside>
</section>
<section id="see-the-schema-1" class="level2">
<h2>See the schema</h2>
<pre><code>messages.printSchema()</code></pre>
</section>
<section id="cast-as-strings" class="level2">
<h2>Cast as strings</h2>
<pre><code>messages_as_strings=messages.selectExpr(&quot;CAST(key AS STRING)&quot;, &quot;CAST(value AS STRING)&quot;)</code></pre>
<aside class="notes">
<p>cast it as strings (you can totally use <code>INT</code>s if you’d like)</p>
<p>messages_as_strings=messages.selectExpr(“CAST(key AS STRING)”, “CAST(value AS STRING)”)</p>
</aside>
</section>
<section id="take-a-look-1" class="level2">
<h2>Take a look</h2>
<pre><code>messages_as_strings.show()</code></pre>
<pre><code>messages_as_strings.printSchema()</code></pre>
<pre><code>messages_as_strings.count()</code></pre>
<aside class="notes">
<p>then you can exit pyspark using either <code>ctrl-d</code> or <code>exit()</code>.</p>
<p><code>messages_as_strings.show()</code></p>
<p>messages_as_strings.printSchema()</p>
<p>messages_as_strings.count()</p>
</aside>
</section>
<section id="unrolling-json" class="level2">
<h2>Unrolling json</h2>
<pre><code>messages_as_strings.select(&#39;value&#39;).take(1)</code></pre>
<pre><code>messages_as_strings.select(&#39;value&#39;).take(1)[0].value</code></pre>
<pre><code>import json</code></pre>
<pre><code>first_message=json.loads(messages_as_strings.select(&#39;value&#39;).take(1)[0].value)</code></pre>
<pre><code>first_message</code></pre>
<pre><code>print(first_message[&#39;commit&#39;][&#39;committer&#39;][&#39;name&#39;])</code></pre>
<aside class="notes">
<p>messages_as_strings.select(‘value’).take(1)</p>
<p>messages_as_strings.select(‘value’).take(1)[0].value &gt;&gt;&gt; import json &gt;&gt;&gt; first_message=json.loads(messages_as_strings.select(‘value’).take(1)[0].value) &gt;&gt;&gt; first_message &gt;&gt;&gt; print(first_message[‘commit’][‘committer’][‘name’]) Nico Williams</p>
</aside>
</section>
<section id="down-1" class="level2">
<h2>Down</h2>
<pre><code>docker-compose down</code></pre>
</section>
</section>
<section id="section-7" class="slide level1">
<h1></h1>
<section id="assignment-07" class="level2">
<h2>Assignment 07</h2>
<ul>
<li>Step through this process using the Project 2 data</li>
<li>What you turn in:</li>
<li>In your <code>/assignment-07-&lt;user-name&gt;</code> repo:
<ul>
<li>your <code>docker-compose.yml</code></li>
<li>once you’ve run the example on your terminal
<ul>
<li>Run <code>history &gt; &lt;user-name&gt;-history.txt</code></li>
<li>Save the relevant portion of your history as <code>&lt;user-name&gt;-annotations.md</code></li>
<li>Annotate the file with explanations of what you were doing at each point (See <code>htmartin-annotations.md</code>)</li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="section-8" class="slide level1">
<h1></h1>
<section id="summary" class="level2">
<h2>Summary</h2>
<p><img data-src="images/streaming-bare.svg" style="border:0;box-shadow:none" /></p>
<aside class="notes">
<ul>
<li>Review what we just did</li>
</ul>
</aside>
</section>
</section>
<section id="section-9" class="slide level1">
<h1></h1>
<p><img class="logo" src="images/berkeley-school-of-information-logo.png"/></p>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Transition style
        transition: 'linear', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
